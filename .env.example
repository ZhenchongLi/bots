# ==============================================
# OpenAI API Proxy Configuration
# ==============================================

# API Configuration
# ----------------------------------
# Your OpenAI API key (required)
OPENAI_API_KEY=your_openai_api_key_here

# OpenAI API base URL - change this to connect to different OpenAI-compatible APIs
# Examples:
#   OpenAI: https://api.openai.com/v1
#   Azure OpenAI: https://your-resource.openai.azure.com/openai/deployments/your-deployment/
#   Local API: http://localhost:8080/v1
OPENAI_BASE_URL=https://api.openai.com/v1

# Server Configuration
# ----------------------------------
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=info

# Database Configuration
# ----------------------------------
# SQLite database for request/response logging
DATABASE_URL=sqlite+aiosqlite:///./logs/proxy.db

# Vector Database Configuration (Optional)
# ----------------------------------
# Qdrant vector database for advanced analytics
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=

# Logging Configuration
# ----------------------------------
LOG_FILE_PATH=./logs/proxy.log
LOG_RETENTION_DAYS=30

# Model Management Configuration
# ----------------------------------
# Comma-separated list of available models
# Default OpenAI models (modify based on your API provider)
AVAILABLE_MODELS=gpt-4,gpt-4-turbo,gpt-4o,gpt-4o-mini,gpt-3.5-turbo,gpt-3.5-turbo-16k,text-embedding-ada-002,text-embedding-3-small,text-embedding-3-large,whisper-1,tts-1,tts-1-hd,dall-e-2,dall-e-3

# Model validation - set to false to allow any model name through
VALIDATE_MODELS=true

# Allow unknown models - set to true to allow models not in AVAILABLE_MODELS
ALLOW_UNKNOWN_MODELS=false

# Model mappings for aliases/compatibility (JSON format)
# This allows you to map custom model names to actual API model names
# Examples:
#   For Claude compatibility: {"claude-3":"gpt-4","claude-2":"gpt-3.5-turbo"}
#   For local models: {"llama2":"gpt-3.5-turbo","codellama":"gpt-4"}
#   For Azure OpenAI: {"gpt-3.5-turbo":"gpt-35-turbo","gpt-4":"gpt-4-32k"}
MODEL_MAPPINGS={}

# ==============================================
# Example configurations for different providers
# ==============================================

# Azure OpenAI Example:
# OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
# OPENAI_API_KEY=your-azure-api-key
# AVAILABLE_MODELS=gpt-35-turbo,gpt-4,text-embedding-ada-002
# MODEL_MAPPINGS={"gpt-3.5-turbo":"gpt-35-turbo"}

# Local LLM Example (e.g., Ollama, LocalAI):
# OPENAI_BASE_URL=http://localhost:11434/v1
# OPENAI_API_KEY=not-needed
# AVAILABLE_MODELS=llama2,codellama,mistral
# VALIDATE_MODELS=false
# ALLOW_UNKNOWN_MODELS=true

# Claude via Bedrock Example:
# OPENAI_BASE_URL=https://your-bedrock-proxy/v1
# OPENAI_API_KEY=your-aws-credentials
# AVAILABLE_MODELS=claude-3-sonnet,claude-3-haiku
# MODEL_MAPPINGS={"gpt-4":"claude-3-sonnet","gpt-3.5-turbo":"claude-3-haiku"}